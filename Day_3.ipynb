{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1xDLC3KbG2pZxnst2X/WY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rawanelmoafy/NLP-Krish-Naik-Playlist/blob/main/Day_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continue With the pragraph we use in Day-2"
      ],
      "metadata": {
        "id": "y-pgFw6a47Yh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uq82OjMd5VJj",
        "outputId": "45874cf5-bb5b-4e47-c066-cd298030b867"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph=\"\"\"The Berners Street hoax was perpetrated by the writer Theodore Hook in Westminster (now part of London) in 1810. After several weeks of preparation he made an apparently spontaneous bet with a friend that he could transform any property into the most talked-about address in London. Hook spent six weeks sending between a thousand and four thousand letters to tradespeople and businesses ordering deliveries of their goods and services to 54 Berners Street, Westminster, at various times on 27 November 1810. Several well-known people were also invited to call on the address, including the chairmen of the Bank of England and the East India Company, the Duke of Gloucester and the Lord Mayor of London.\n",
        "\n",
        "Hook and his friends rented rooms in the house opposite number 54 to view proceedings. Chimney sweeps began arriving at the address at 5:00 am on the day, followed by hundreds of representatives of several trades and businesses, including auctioneers, undertakers, grocers, butchers, bakers, pastry chefs and dancing masters; goods deliveries included organs, furniture, coal, wedding cakes, food, drink and a coffin. The police were called to try and manage the crowd but they were not able to clear the street until after the final influx of visitors at 5:00 pm: domestic servants who thought they were to be interviewed for a job.\n",
        "\n",
        "Hook was unidentified at the time, but admitted his involvement in a semi-autobiographical novel published twenty-five years after the event. The hoax was repeated across Britain and Paris, and was retold on stage, in song and by cartoonists.\"\"\""
      ],
      "metadata": {
        "id": "DCM4WfhN5Snj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "QrV5fxrc6Fsu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer =PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sij-4LeQ6KH0",
        "outputId": "6b3a8753-6228-4aab-c2de-ed523972e456"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = []\n",
        "for i in range(len(sentences)):\n",
        "  text = re.sub('[^a-zA-Z]',' ',sentences[i])\n",
        "  text = text.lower()\n",
        "  text = text.split()\n",
        "  text = [lemmatizer.lemmatize(word) for word in text if not word in set(stopwords.words('english')) ]\n",
        "  text = ' '.join(text)\n",
        "  corpus.append(text)\n",
        "corpus[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "TQHsa8zq5_MZ",
        "outputId": "fa7169cd-1dbc-4ada-d5e3-9c5db79d0ea9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'berners street hoax perpetrated writer theodore hook westminster part london'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BOW using ngram_range\n",
        "ngram_range = (3,3) -> that's will take trigrams\n",
        "ngram_range = (2,3) -> will take bigrams and trigrams"
      ],
      "metadata": {
        "id": "caEhDWq76Y1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CV = CountVectorizer(binary = True, ngram_range=(3,3))\n",
        "X = CV.fit_transform(corpus)\n",
        "CV.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AG5Qx0pR6YZ8",
        "outputId": "8bce592f-132d-4f61-feef-813fe00ed905"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'berners street hoax': 13,\n",
              " 'street hoax perpetrated': 103,\n",
              " 'hoax perpetrated writer': 51,\n",
              " 'perpetrated writer theodore': 80,\n",
              " 'writer theodore hook': 127,\n",
              " 'theodore hook westminster': 107,\n",
              " 'hook westminster part': 56,\n",
              " 'westminster part london': 125,\n",
              " 'several week preparation': 96,\n",
              " 'week preparation made': 122,\n",
              " 'preparation made apparently': 83,\n",
              " 'made apparently spontaneous': 69,\n",
              " 'apparently spontaneous bet': 6,\n",
              " 'spontaneous bet friend': 100,\n",
              " 'bet friend could': 15,\n",
              " 'friend could transform': 44,\n",
              " 'could transform property': 29,\n",
              " 'transform property talked': 114,\n",
              " 'property talked address': 84,\n",
              " 'talked address london': 106,\n",
              " 'hook spent six': 54,\n",
              " 'spent six week': 99,\n",
              " 'six week sending': 98,\n",
              " 'week sending thousand': 123,\n",
              " 'sending thousand four': 92,\n",
              " 'thousand four thousand': 109,\n",
              " 'four thousand letter': 43,\n",
              " 'thousand letter tradespeople': 110,\n",
              " 'letter tradespeople business': 67,\n",
              " 'tradespeople business ordering': 113,\n",
              " 'business ordering delivery': 18,\n",
              " 'ordering delivery good': 75,\n",
              " 'delivery good service': 33,\n",
              " 'good service berners': 49,\n",
              " 'service berners street': 94,\n",
              " 'berners street westminster': 14,\n",
              " 'street westminster various': 104,\n",
              " 'westminster various time': 126,\n",
              " 'various time november': 119,\n",
              " 'several well known': 97,\n",
              " 'well known people': 124,\n",
              " 'known people also': 66,\n",
              " 'people also invited': 79,\n",
              " 'also invited call': 5,\n",
              " 'invited call address': 64,\n",
              " 'call address including': 21,\n",
              " 'address including chairman': 3,\n",
              " 'including chairman bank': 61,\n",
              " 'chairman bank england': 23,\n",
              " 'bank england east': 11,\n",
              " 'england east india': 38,\n",
              " 'east india company': 37,\n",
              " 'india company duke': 62,\n",
              " 'company duke gloucester': 28,\n",
              " 'duke gloucester lord': 36,\n",
              " 'gloucester lord mayor': 47,\n",
              " 'lord mayor london': 68,\n",
              " 'hook friend rented': 53,\n",
              " 'friend rented room': 45,\n",
              " 'rented room house': 86,\n",
              " 'room house opposite': 90,\n",
              " 'house opposite number': 57,\n",
              " 'opposite number view': 74,\n",
              " 'number view proceeding': 73,\n",
              " 'chimney sweep began': 25,\n",
              " 'sweep began arriving': 105,\n",
              " 'began arriving address': 12,\n",
              " 'arriving address day': 7,\n",
              " 'address day followed': 2,\n",
              " 'day followed hundred': 32,\n",
              " 'followed hundred representative': 41,\n",
              " 'hundred representative several': 58,\n",
              " 'representative several trade': 88,\n",
              " 'several trade business': 95,\n",
              " 'trade business including': 112,\n",
              " 'business including auctioneer': 17,\n",
              " 'including auctioneer undertaker': 60,\n",
              " 'auctioneer undertaker grocer': 8,\n",
              " 'undertaker grocer butcher': 117,\n",
              " 'grocer butcher baker': 50,\n",
              " 'butcher baker pastry': 19,\n",
              " 'baker pastry chef': 10,\n",
              " 'pastry chef dancing': 78,\n",
              " 'chef dancing master': 24,\n",
              " 'dancing master good': 31,\n",
              " 'master good delivery': 71,\n",
              " 'good delivery included': 48,\n",
              " 'delivery included organ': 34,\n",
              " 'included organ furniture': 59,\n",
              " 'organ furniture coal': 76,\n",
              " 'furniture coal wedding': 46,\n",
              " 'coal wedding cake': 27,\n",
              " 'wedding cake food': 121,\n",
              " 'cake food drink': 20,\n",
              " 'food drink coffin': 42,\n",
              " 'police called try': 82,\n",
              " 'called try manage': 22,\n",
              " 'try manage crowd': 115,\n",
              " 'manage crowd able': 70,\n",
              " 'crowd able clear': 30,\n",
              " 'able clear street': 0,\n",
              " 'clear street final': 26,\n",
              " 'street final influx': 102,\n",
              " 'final influx visitor': 39,\n",
              " 'influx visitor pm': 63,\n",
              " 'visitor pm domestic': 120,\n",
              " 'pm domestic servant': 81,\n",
              " 'domestic servant thought': 35,\n",
              " 'servant thought interviewed': 93,\n",
              " 'thought interviewed job': 108,\n",
              " 'hook unidentified time': 55,\n",
              " 'unidentified time admitted': 118,\n",
              " 'time admitted involvement': 111,\n",
              " 'admitted involvement semi': 4,\n",
              " 'involvement semi autobiographical': 65,\n",
              " 'semi autobiographical novel': 91,\n",
              " 'autobiographical novel published': 9,\n",
              " 'novel published twenty': 72,\n",
              " 'published twenty five': 85,\n",
              " 'twenty five year': 116,\n",
              " 'five year event': 40,\n",
              " 'hoax repeated across': 52,\n",
              " 'repeated across britain': 87,\n",
              " 'across britain paris': 1,\n",
              " 'britain paris retold': 16,\n",
              " 'paris retold stage': 77,\n",
              " 'retold stage song': 89,\n",
              " 'stage song cartoonist': 101}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CV = CountVectorizer(binary = True, ngram_range=(2,3))\n",
        "X = CV.fit_transform(corpus)\n",
        "CV.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkS2kIvO7SPu",
        "outputId": "e96b18b2-f2d7-4380-80b2-a3448ee0b51d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'berners street': 27,\n",
              " 'street hoax': 211,\n",
              " 'hoax perpetrated': 103,\n",
              " 'perpetrated writer': 164,\n",
              " 'writer theodore': 261,\n",
              " 'theodore hook': 219,\n",
              " 'hook westminster': 113,\n",
              " 'westminster part': 257,\n",
              " 'part london': 159,\n",
              " 'berners street hoax': 28,\n",
              " 'street hoax perpetrated': 212,\n",
              " 'hoax perpetrated writer': 104,\n",
              " 'perpetrated writer theodore': 165,\n",
              " 'writer theodore hook': 262,\n",
              " 'theodore hook westminster': 220,\n",
              " 'hook westminster part': 114,\n",
              " 'westminster part london': 258,\n",
              " 'several week': 196,\n",
              " 'week preparation': 251,\n",
              " 'preparation made': 170,\n",
              " 'made apparently': 140,\n",
              " 'apparently spontaneous': 13,\n",
              " 'spontaneous bet': 205,\n",
              " 'bet friend': 30,\n",
              " 'friend could': 89,\n",
              " 'could transform': 58,\n",
              " 'transform property': 234,\n",
              " 'property talked': 172,\n",
              " 'talked address': 217,\n",
              " 'address london': 8,\n",
              " 'several week preparation': 197,\n",
              " 'week preparation made': 252,\n",
              " 'preparation made apparently': 171,\n",
              " 'made apparently spontaneous': 141,\n",
              " 'apparently spontaneous bet': 14,\n",
              " 'spontaneous bet friend': 206,\n",
              " 'bet friend could': 31,\n",
              " 'friend could transform': 90,\n",
              " 'could transform property': 59,\n",
              " 'transform property talked': 235,\n",
              " 'property talked address': 173,\n",
              " 'talked address london': 218,\n",
              " 'hook spent': 109,\n",
              " 'spent six': 203,\n",
              " 'six week': 200,\n",
              " 'week sending': 253,\n",
              " 'sending thousand': 188,\n",
              " 'thousand four': 223,\n",
              " 'four thousand': 87,\n",
              " 'thousand letter': 225,\n",
              " 'letter tradespeople': 136,\n",
              " 'tradespeople business': 232,\n",
              " 'business ordering': 36,\n",
              " 'ordering delivery': 153,\n",
              " 'delivery good': 66,\n",
              " 'good service': 99,\n",
              " 'service berners': 192,\n",
              " 'street westminster': 213,\n",
              " 'westminster various': 259,\n",
              " 'various time': 244,\n",
              " 'time november': 229,\n",
              " 'hook spent six': 110,\n",
              " 'spent six week': 204,\n",
              " 'six week sending': 201,\n",
              " 'week sending thousand': 254,\n",
              " 'sending thousand four': 189,\n",
              " 'thousand four thousand': 224,\n",
              " 'four thousand letter': 88,\n",
              " 'thousand letter tradespeople': 226,\n",
              " 'letter tradespeople business': 137,\n",
              " 'tradespeople business ordering': 233,\n",
              " 'business ordering delivery': 37,\n",
              " 'ordering delivery good': 154,\n",
              " 'delivery good service': 67,\n",
              " 'good service berners': 100,\n",
              " 'service berners street': 193,\n",
              " 'berners street westminster': 29,\n",
              " 'street westminster various': 214,\n",
              " 'westminster various time': 260,\n",
              " 'various time november': 245,\n",
              " 'several well': 198,\n",
              " 'well known': 255,\n",
              " 'known people': 134,\n",
              " 'people also': 162,\n",
              " 'also invited': 11,\n",
              " 'invited call': 130,\n",
              " 'call address': 42,\n",
              " 'address including': 6,\n",
              " 'including chairman': 123,\n",
              " 'chairman bank': 46,\n",
              " 'bank england': 23,\n",
              " 'england east': 77,\n",
              " 'east india': 75,\n",
              " 'india company': 125,\n",
              " 'company duke': 56,\n",
              " 'duke gloucester': 73,\n",
              " 'gloucester lord': 95,\n",
              " 'lord mayor': 138,\n",
              " 'mayor london': 146,\n",
              " 'several well known': 199,\n",
              " 'well known people': 256,\n",
              " 'known people also': 135,\n",
              " 'people also invited': 163,\n",
              " 'also invited call': 12,\n",
              " 'invited call address': 131,\n",
              " 'call address including': 43,\n",
              " 'address including chairman': 7,\n",
              " 'including chairman bank': 124,\n",
              " 'chairman bank england': 47,\n",
              " 'bank england east': 24,\n",
              " 'england east india': 78,\n",
              " 'east india company': 76,\n",
              " 'india company duke': 126,\n",
              " 'company duke gloucester': 57,\n",
              " 'duke gloucester lord': 74,\n",
              " 'gloucester lord mayor': 96,\n",
              " 'lord mayor london': 139,\n",
              " 'hook friend': 107,\n",
              " 'friend rented': 91,\n",
              " 'rented room': 176,\n",
              " 'room house': 184,\n",
              " 'house opposite': 115,\n",
              " 'opposite number': 151,\n",
              " 'number view': 149,\n",
              " 'view proceeding': 246,\n",
              " 'hook friend rented': 108,\n",
              " 'friend rented room': 92,\n",
              " 'rented room house': 177,\n",
              " 'room house opposite': 185,\n",
              " 'house opposite number': 116,\n",
              " 'opposite number view': 152,\n",
              " 'number view proceeding': 150,\n",
              " 'chimney sweep': 50,\n",
              " 'sweep began': 215,\n",
              " 'began arriving': 25,\n",
              " 'arriving address': 15,\n",
              " 'address day': 4,\n",
              " 'day followed': 64,\n",
              " 'followed hundred': 83,\n",
              " 'hundred representative': 117,\n",
              " 'representative several': 180,\n",
              " 'several trade': 194,\n",
              " 'trade business': 230,\n",
              " 'business including': 34,\n",
              " 'including auctioneer': 121,\n",
              " 'auctioneer undertaker': 17,\n",
              " 'undertaker grocer': 240,\n",
              " 'grocer butcher': 101,\n",
              " 'butcher baker': 38,\n",
              " 'baker pastry': 21,\n",
              " 'pastry chef': 160,\n",
              " 'chef dancing': 48,\n",
              " 'dancing master': 62,\n",
              " 'master good': 144,\n",
              " 'good delivery': 97,\n",
              " 'delivery included': 68,\n",
              " 'included organ': 119,\n",
              " 'organ furniture': 155,\n",
              " 'furniture coal': 93,\n",
              " 'coal wedding': 54,\n",
              " 'wedding cake': 249,\n",
              " 'cake food': 40,\n",
              " 'food drink': 85,\n",
              " 'drink coffin': 72,\n",
              " 'chimney sweep began': 51,\n",
              " 'sweep began arriving': 216,\n",
              " 'began arriving address': 26,\n",
              " 'arriving address day': 16,\n",
              " 'address day followed': 5,\n",
              " 'day followed hundred': 65,\n",
              " 'followed hundred representative': 84,\n",
              " 'hundred representative several': 118,\n",
              " 'representative several trade': 181,\n",
              " 'several trade business': 195,\n",
              " 'trade business including': 231,\n",
              " 'business including auctioneer': 35,\n",
              " 'including auctioneer undertaker': 122,\n",
              " 'auctioneer undertaker grocer': 18,\n",
              " 'undertaker grocer butcher': 241,\n",
              " 'grocer butcher baker': 102,\n",
              " 'butcher baker pastry': 39,\n",
              " 'baker pastry chef': 22,\n",
              " 'pastry chef dancing': 161,\n",
              " 'chef dancing master': 49,\n",
              " 'dancing master good': 63,\n",
              " 'master good delivery': 145,\n",
              " 'good delivery included': 98,\n",
              " 'delivery included organ': 69,\n",
              " 'included organ furniture': 120,\n",
              " 'organ furniture coal': 156,\n",
              " 'furniture coal wedding': 94,\n",
              " 'coal wedding cake': 55,\n",
              " 'wedding cake food': 250,\n",
              " 'cake food drink': 41,\n",
              " 'food drink coffin': 86,\n",
              " 'police called': 168,\n",
              " 'called try': 44,\n",
              " 'try manage': 236,\n",
              " 'manage crowd': 142,\n",
              " 'crowd able': 60,\n",
              " 'able clear': 0,\n",
              " 'clear street': 52,\n",
              " 'street final': 209,\n",
              " 'final influx': 79,\n",
              " 'influx visitor': 127,\n",
              " 'visitor pm': 247,\n",
              " 'pm domestic': 166,\n",
              " 'domestic servant': 70,\n",
              " 'servant thought': 190,\n",
              " 'thought interviewed': 221,\n",
              " 'interviewed job': 129,\n",
              " 'police called try': 169,\n",
              " 'called try manage': 45,\n",
              " 'try manage crowd': 237,\n",
              " 'manage crowd able': 143,\n",
              " 'crowd able clear': 61,\n",
              " 'able clear street': 1,\n",
              " 'clear street final': 53,\n",
              " 'street final influx': 210,\n",
              " 'final influx visitor': 80,\n",
              " 'influx visitor pm': 128,\n",
              " 'visitor pm domestic': 248,\n",
              " 'pm domestic servant': 167,\n",
              " 'domestic servant thought': 71,\n",
              " 'servant thought interviewed': 191,\n",
              " 'thought interviewed job': 222,\n",
              " 'hook unidentified': 111,\n",
              " 'unidentified time': 242,\n",
              " 'time admitted': 227,\n",
              " 'admitted involvement': 9,\n",
              " 'involvement semi': 132,\n",
              " 'semi autobiographical': 186,\n",
              " 'autobiographical novel': 19,\n",
              " 'novel published': 147,\n",
              " 'published twenty': 174,\n",
              " 'twenty five': 238,\n",
              " 'five year': 81,\n",
              " 'year event': 263,\n",
              " 'hook unidentified time': 112,\n",
              " 'unidentified time admitted': 243,\n",
              " 'time admitted involvement': 228,\n",
              " 'admitted involvement semi': 10,\n",
              " 'involvement semi autobiographical': 133,\n",
              " 'semi autobiographical novel': 187,\n",
              " 'autobiographical novel published': 20,\n",
              " 'novel published twenty': 148,\n",
              " 'published twenty five': 175,\n",
              " 'twenty five year': 239,\n",
              " 'five year event': 82,\n",
              " 'hoax repeated': 105,\n",
              " 'repeated across': 178,\n",
              " 'across britain': 2,\n",
              " 'britain paris': 32,\n",
              " 'paris retold': 157,\n",
              " 'retold stage': 182,\n",
              " 'stage song': 207,\n",
              " 'song cartoonist': 202,\n",
              " 'hoax repeated across': 106,\n",
              " 'repeated across britain': 179,\n",
              " 'across britain paris': 3,\n",
              " 'britain paris retold': 33,\n",
              " 'paris retold stage': 158,\n",
              " 'retold stage song': 183,\n",
              " 'stage song cartoonist': 208}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X[0].toarray()\n",
        "# the size increase compare we used just one toke like in Day-2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orhAKcaP7eXi",
        "outputId": "5f488cdd-5b1b-4a50-ad43-58fb30e30c2f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
              "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF"
      ],
      "metadata": {
        "id": "4BH_QQ7Q3wsK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1dej3bZOxlqf"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "cv = TfidfVectorizer()\n",
        "X = cv.fit_transform(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "IAsxQMTC4Adw",
        "outputId": "0caf30ad-8ef6-4d47-a271-9970e6cb5f64"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'berners street hoax perpetrated writer theodore hook westminster part london'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X[0].toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZWbM9FL7-04",
        "outputId": "99a8ecf8-3dbd-4c61-dbdb-be1ade9fa893"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.30557657, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.30557657,\n",
              "        0.23475158, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.26569001, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.36179352, 0.        , 0.        , 0.36179352,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.26569001, 0.        , 0.        , 0.36179352,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.30557657, 0.36179352, 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try ngrams with TF-IDF\n",
        "cv = TfidfVectorizer(ngram_range=(2,3))\n",
        "X = cv.fit_transform(corpus)\n",
        "X[0].toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYUJa3gZ8EGX",
        "outputId": "64838cd6-0ccd-4374-fcb8-d24c53f5f57d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.2065985 , 0.24460645, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.24460645, 0.24460645,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.24460645, 0.24460645,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.24460645,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.24460645,\n",
              "        0.24460645, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.24460645, 0.24460645, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.24460645,\n",
              "        0.24460645, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.24460645, 0.24460645, 0.        ,\n",
              "        0.        , 0.24460645, 0.24460645, 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv = TfidfVectorizer(max_features=3)\n",
        "#max_features-> that means taking the top 3 words by their weights and keeping them in the vocabulary\n",
        "X = cv.fit_transform(corpus)\n",
        "X[0].toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUjsI6KT89St",
        "outputId": "0ca981ff-d642-4d83-a540-43fcac6c026b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.66212774, 0.74939099]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sM3g4Jtk-Mhc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}